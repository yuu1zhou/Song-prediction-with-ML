{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2ebf80",
   "metadata": {},
   "source": [
    "# PCA section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7052b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_selected_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18787371",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var_lis = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim_lis = np.array(range(76)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_idx= np.argmax(explained_var_lis >= 0.95) + 1\n",
    "print(\"elbow index: \", elbow_idx)\n",
    "elbow_value = explained_var_lis[elbow_idx - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dim_lis,explained_var_lis, linewidth=3, color = \"maroon\")\n",
    "ax.set_ylabel(\"Explained variance\")\n",
    "ax.set_xlabel(\"Dimensions\")\n",
    "\n",
    "plt.scatter(elbow_idx, elbow_value, color = \"green\", s =100)\n",
    "ax.plot([elbow_idx, elbow_idx], [0, elbow_value], linestyle = '--', color = 'grey')\n",
    "ax.plot([0,elbow_idx], [elbow_value, elbow_value], linestyle = '--', color = \"grey\")\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da3865",
   "metadata": {},
   "source": [
    "Therefore we reduced original data dimension to 189 by applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc905996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = PCA(n_components= 38).fit_transform(X_train_selected_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f809291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41915263",
   "metadata": {},
   "source": [
    "# Forward step-wise section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54535d5f",
   "metadata": {},
   "source": [
    "However, after visualizing both the cross-validation and training errors against the number of features, we observed that the model's cross-validation error started to gradually decrease around 170 features and began to increase around 240 features, indicating the sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b9354",
   "metadata": {},
   "source": [
    "Based on the zoom-in plot, we observed that the cross-validation error decreased slowly, indicating that increasing the number of features provided little improvement to model performance. So we decideed to set the number of selected forward stepwise features to 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d77da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(feature_num_lis, training_error_lis, label= \"training error\", color = \"navy\")\n",
    "ax.plot(feature_num_lis, cv_error_lis, label = \"cross validation error\", color = \"magenta\")\n",
    "ax.set(xlim=(120,190))\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Number of features\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cf549",
   "metadata": {},
   "source": [
    "# Clustering section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cluster_oof_features(X_stats, y, k=32, n_splits=5, task=\"classification\"):\n",
    "    n = len(y)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_dist  = np.zeros((n, k)) # distance to core of each cluster, each song\n",
    "    oof_mean  = np.zeros((n, 1)) # mean, each song\n",
    "    oof_size  = np.zeros((n, 1)) # \n",
    "    oof_label = np.zeros(n, dtype=int) # cluster labrl, each song\n",
    "\n",
    "    # 为了在测试时使用：训练完后再用全量拟合一遍\n",
    "    #scaler_full = StandardScaler().fit(X_stats)\n",
    "    #Xz_full = scaler_full.transform(X_stats)\n",
    "    #km_full = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=2048, n_init=\"auto\").fit(Xz_full)\n",
    "\n",
    "    for tr, va in kf.split(X_stats): # training, validation sets\n",
    "        # 1) 该折的标准化 + 聚类只在训练折上拟合\n",
    "        scaler = StandardScaler().fit(X_stats[tr])\n",
    "        Xz_tr = scaler.transform(X_stats[tr])\n",
    "        Xz_va = scaler.transform(X_stats[va])\n",
    "\n",
    "        km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=2048, n_init=\"auto\").fit(Xz_tr)\n",
    "\n",
    "        # 2) 给验证折打特征\n",
    "        oof_dist[va]  = km.transform(Xz_va)        # 到质心距离（越小越近）\n",
    "        oof_label[va] = km.predict(Xz_va)\n",
    "\n",
    "        # 3) 只用训练折按簇统计目标\n",
    "        #lab_tr = km.predict(Xz_tr)\n",
    "        #mean_by_c = {}  # 每一个cluster 的 mean\n",
    "        #size_by_c = {} # 每一个cluster 的 size\n",
    "        #base = y[tr].mean()\n",
    "        #for c in range(k):\n",
    "        #    mask = (lab_tr == c) \n",
    "        #    size = int(mask.sum())\n",
    "        #    size_by_c[c] = size #每一个 label 的 size, training set\n",
    "        #    if size > 0:\n",
    "        #        m = y[tr][mask].mean() if task == \"regression\" else y[tr][mask].mean()\n",
    "        #    else:\n",
    "        #        m = base\n",
    "        #    mean_by_c[c] = m\n",
    "\n",
    "        # 4) 把簇统计赋给验证折样本\n",
    "        #oof_mean[va, 0] = np.array([mean_by_c[c] for c in oof_label[va]])\n",
    "        #oof_size[va, 0] = np.array([size_by_c[c] for c in oof_label[va]])\n",
    "\n",
    "    # 供测试阶段：用全训练集的簇统计\n",
    "    #lab_full = km_full.predict(Xz_full)\n",
    "    #full_mean = {c: (y[lab_full==c].mean() if (lab_full==c).sum()>0 else y.mean()) for c in range(k)}\n",
    "    #full_size = {c: (lab_full==c).sum() for c in range(k)}\n",
    "\n",
    "    #cluster_stats_full = {\"mean\": full_mean, \"size\": full_size}\n",
    "\n",
    "    return (oof_dist, #oof_mean, oof_size, \n",
    "            oof_label,\n",
    "            #scaler_full, km_full, cluster_stats_full\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5eab44",
   "metadata": {},
   "source": [
    "# acoustic feature section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_matrix_bars = np.concatenate((array_cluster_bars[0], array_cluster_bars[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_bars.shape\n",
    "\n",
    "combined_matrix_beats = np.concatenate((array_cluster_beats[0], array_cluster_beats[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_beats.shape\n",
    "\n",
    "combined_matrix_sections = np.concatenate((array_cluster_sections[0], array_cluster_sections[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_sections.shape\n",
    "\n",
    "combined_matrix_loudness = np.concatenate((array_cluster_loudness[0], array_cluster_loudness[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_loudness.shape\n",
    "\n",
    "combined_matrix_loudness_time = np.concatenate((array_cluster_loudness_time[0], array_cluster_loudness_time[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_loudness_time.shape\n",
    "\n",
    "combined_matrix_loudness_start = np.concatenate((array_cluster_loudness_start[0], array_cluster_loudness_start[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_loudness_start.shape\n",
    "\n",
    "combined_matrix_segments_start = np.concatenate((array_cluster_segments_start[0], array_cluster_segments_start[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_segments_start.shape\n",
    "\n",
    "combined_matrix_tatums_start = np.concatenate((array_cluster_tatums_start[0], array_cluster_tatums_start[1].reshape(-1,1)), axis=1) #final matrix but not standardized yet\n",
    "combined_matrix_tatums_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_matrix_bars.shape, combined_matrix_beats.shape, combined_matrix_sections.shape,combined_matrix_loudness.shape, combined_matrix_loudness_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matrix_combiend = np.concatenate([combined_matrix_bars, combined_matrix_beats, combined_matrix_sections, combined_matrix_loudness, \n",
    "                                      combined_matrix_loudness_time\n",
    "                                      ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"acoustic_features_part.npy\", all_matrix_combiend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "all_matrix_combiend = scaler.fit_transform(all_matrix_combiend)\n",
    "\n",
    "np.save(\"acoustic_features_standardized_part.npy\", all_matrix_combiend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distance_bars = array_cluster_bars[0]\n",
    "cluster_distance_beats = array_cluster_beats[0]\n",
    "cluster_distance_loudness = array_cluster_loudness[0]\n",
    "cluster_distance_loudness_time = array_cluster_loudness_time[0]\n",
    "\n",
    "np.save(\"cluster_distance_bars\", cluster_distance_bars)\n",
    "np.save(\"cluster_distance_beats\", cluster_distance_beats)\n",
    "np.save(\"cluster_distance_loudness\", cluster_distance_loudness)\n",
    "np.save(\"cluster_distance_loudness_time\", cluster_distance_loudness_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
